# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ExplainVulnerability < Gitlab::Llm::Completions::Base
        DEFAULT_ERROR = 'An unexpected error has occurred.'
        NULL_PROMPT_ERROR = 'Empty prompt error. The vulnerability may be ' \
                            'incomptible with the `include_source_code` parameter.'

        def execute(user, vulnerability, options)
          response = response_for(user, vulnerability, options)
        rescue StandardError => error
          Gitlab::ErrorTracking.track_exception(error)

          response = formatted_error_response(DEFAULT_ERROR)
        ensure
          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute
        end

        private

        def formatted_error_response(message)
          { error: { message: message } }.to_json
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(vulnerability, options), expires_in: 5.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt
            if prompt.to_s.empty?
              formatted_error_response(NULL_PROMPT_ERROR)
            else
              request(user, prompt, vulnerability)
            end
          end
        end

        # There is some code duplication for the evaluation for different models.
        # While it would be cleaner to seperate these, once a model is settled upon, additional files
        # will be redundant and deleted. This is a lower effort approach for code that will ultimately
        # be disposed of.

        def modify_response(response, vulnerability)
          if anthropic?(vulnerability)
            ::Gitlab::Llm::Anthropic::ResponseModifiers::ExplainVulnerability.new(response)
          else
            ::Gitlab::Llm::VertexAi::ResponseModifiers::Predictions.new(response)
          end
        end

        def request(user, prompt, vulnerability)
          if anthropic?(vulnerability)
            client_class = ::Gitlab::Llm::Anthropic::Client
            client_class
              .new(user)
              .complete(prompt: "\n\nHuman: #{prompt}\n\nAssistant:")
          else
            client_class = ::Gitlab::Llm::VertexAi::Client
            client_class
              .new(user, retry_content_blocked_requests: true)
              .text(content: prompt)
          end
        end

        def anthropic?(vulnerability)
          Feature.enabled?(:explain_vulnerability_anthropic, vulnerability.project)
        end

        def cache_key(vulnerability, options)
          include_source_code_key = "include_source_code:#{options[:include_source_code].inspect}"

          [vulnerability.cache_key, 'explain', include_source_code_key].join('/')
        end
      end
    end
  end
end
