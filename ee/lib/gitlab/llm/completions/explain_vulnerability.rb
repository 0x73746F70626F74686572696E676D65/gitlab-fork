# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ExplainVulnerability < Gitlab::Llm::Completions::Base
        DEFAULT_ERROR = 'An unexpected error has occurred.'

        def execute(user, vulnerability, options)
          response = response_for(user, vulnerability, options)
        rescue StandardError => error
          Gitlab::ErrorTracking.track_exception(error)

          response = { error: { message: DEFAULT_ERROR } }.to_json
        ensure
          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute
        end

        private

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(vulnerability, options), expires_in: 5.minutes, skip_nil: true) do
            template = ai_prompt_class.new(vulnerability, options)
            request(user, template, vulnerability)
          end
        end

        # There is some code duplication for the evaluation for different models.
        # While it would be cleaner to seperate these, once a model is settled upon, additional files
        # will be redundant and deleted. This is a lower effort approach for code that will ultimately
        # be disposed of.

        def modify_response(response, vulnerability)
          if anthropic?(vulnerability)
            ::Gitlab::Llm::Anthropic::ResponseModifiers::ExplainVulnerability.new(response)
          else
            ::Gitlab::Llm::VertexAi::ResponseModifiers::Predictions.new(response)
          end
        end

        def request(user, template, vulnerability)
          if anthropic?(vulnerability)
            client_class = ::Gitlab::Llm::Anthropic::Client
            client_class
              .new(user)
              .complete(prompt: "\n\nHuman: #{template.to_prompt}\n\nAssistant:", **template.options(client_class))
          else
            client_class = ::Gitlab::Llm::VertexAi::Client
            client_class
              .new(user, retry_content_blocked_requests: true)
              .text(content: template.to_prompt, **template.options(client_class))
          end
        end

        def anthropic?(vulnerability)
          Feature.enabled?(:explain_vulnerability_anthropic, vulnerability.project)
        end

        def cache_key(vulnerability, options)
          include_source_code_key = "include_source_code:#{options[:include_source_code].inspect}"

          [vulnerability.cache_key, 'explain', include_source_code_key].join('/')
        end
      end
    end
  end
end
