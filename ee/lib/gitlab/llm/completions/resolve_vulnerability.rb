# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ResolveVulnerability < Gitlab::Llm::Completions::Base
        DEFAULT_ERROR = 'An unexpected error has occurred.'
        CLIENT_TIMEOUT_ERROR = 'The upstream AI provider request timed out without responding.'
        RESOLUTION_FAILURE_ERROR = 'Something went wrong while attempting to apply the ' \
                                   'AI resolution to a merge request.'

        # Extract the triple ` quoted code from the llm response with a named capture group called "change".
        # Important that the `.+` syntax be permitted to match over newlines.
        LLM_DIFF_REGEX = /```\n(?<change>.+)\n```/ms

        def execute
          ai_response, diff_extracted = response_for(user, vulnerability, options)

          response = diff_extracted ? create_merge_request(user, vulnerability, ai_response) : ai_response
          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        rescue StandardError => error
          Gitlab::ErrorTracking.track_exception(error)

          response = formatted_error_response(error_message(error))
          response_modifier = modify_response(response, vulnerability)
          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(user, vulnerability), expires_in: 10.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt

            extract_llm_change(request(user, prompt, vulnerability))
          end
        end

        private

        def extract_llm_change(ai_response)
          match_data = ai_response.dig('predictions', 0, 'content').to_s.match(LLM_DIFF_REGEX)
          return [ai_response, false] unless match_data

          [match_data[:change], true]
        end

        def create_merge_request(user, vulnerability, response)
          return unless response.present? && !@null_prompt_error

          result = ::MergeRequests::CreateFromVulnerabilityDataService.new(
            vulnerability.project,
            user,
            llm_patch: response,
            vulnerability: vulnerability
          ).execute

          return formatted_error_response(RESOLUTION_FAILURE_ERROR, context: result) unless result[:status] == :success

          { merge_request_reference: result[:merge_request].to_reference }
        end

        def error_message(error)
          return CLIENT_TIMEOUT_ERROR if error.is_a?(Net::ReadTimeout)

          DEFAULT_ERROR
        end

        def formatted_error_response(message, context = {})
          { error: { message: message, context: context } }.to_json
        end

        def modify_response(response, _vulnerability)
          ::Gitlab::Llm::ResponseModifiers::ResolveVulnerability.new(response)
        end

        def request(user, prompt, _vulnerability)
          ::Gitlab::Llm::VertexAi::Client.new(user, tracking_context: tracking_context).code(content: prompt)
        end

        def cache_key(user, vulnerability)
          [user.id, vulnerability.cache_key, 'resolve'].join('/')
        end

        def vulnerability
          resource
        end
      end
    end
  end
end
