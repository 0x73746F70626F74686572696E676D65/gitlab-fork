# frozen_string_literal: true

module Gitlab
  module Llm
    module VertexAi
      module Completions
        class ExplainVulnerability
          DEFAULT_ERROR = 'An unexpected error has occurred.'

          def initialize(template_class)
            @template_class = template_class
          end

          def execute(user, vulnerability, options)
            unless vertex_ai?(vulnerability)
              return ::Gitlab::Llm::OpenAi::Completions::ExplainVulnerability
                .new(template_class)
                .execute(user, vulnerability, options)
            end

            template = template_class.new(vulnerability)
            response = response_for(user, template)

            json = Gitlab::Json.parse(response, symbolize_names: true)

            GraphqlTriggers.ai_completion_response(
              user.to_global_id,
              vulnerability.to_global_id,
              {
                id: SecureRandom.uuid,
                model_name: vulnerability.class.name,
                response_body: json.dig(:predictions, 0, :candidates, 0, :content),
                errors: [json.dig(:error, :message)].compact
              }
            )
          rescue StandardError => error
            Gitlab::ErrorTracking.track_exception(error)

            ::Gitlab::Llm::OpenAi::ResponseService
              .new(user, vulnerability, { error: { message: DEFAULT_ERROR } }.to_json, options: {})
              .execute
          end

          private

          attr_reader :template_class

          def response_for(user, template)
            client_class = ::Gitlab::Llm::VertexAi::Client
            client_class
              .new(user)
              .chat(content: template.to_prompt, **template.options(client_class))
          end

          def vertex_ai?(vulnerability)
            Feature.enabled?(:tofa_experimentation, vulnerability.project)
          end
        end
      end
    end
  end
end
