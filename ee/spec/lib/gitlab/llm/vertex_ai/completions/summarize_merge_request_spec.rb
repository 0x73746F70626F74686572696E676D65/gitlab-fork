# frozen_string_literal: true

require 'spec_helper'

RSpec.describe Gitlab::Llm::VertexAi::Completions::SummarizeMergeRequest, feature_category: :code_review_workflow do
  let_it_be(:project) { create(:project, :repository) }
  let_it_be(:merge_request) { create(:merge_request, source_project: project, target_project: project) }
  let_it_be(:mr_diff) { merge_request.merge_request_diff }
  let_it_be(:user) { merge_request.author }

  let(:prompt_class) { Gitlab::Llm::Templates::SummarizeMergeRequest }
  let(:diff_id) { mr_diff.id }
  let(:tracking_context) { { action: :summarize_merge_request, request_id: 'uuid' } }
  let(:options) do
    {
      diff_id: diff_id,
      action: :summarize_merge_request
    }
  end

  let(:prompt_message) do
    build(:ai_message, :summarize_merge_request, user: user, resource: merge_request, request_id: 'uuid')
  end

  subject { described_class.new(prompt_message, prompt_class, options) }

  describe '#execute' do
    let(:prompt) { "This is a prompt." }

    let(:payload_parameters) do
      {
        temperature: 0,
        maxOutputTokens: 1024,
        topK: 40,
        topP: 0.95
      }
    end

    before do
      allow_next_instance_of(prompt_class) do |template|
        allow(template).to receive(:to_prompt).and_return(prompt)
      end

      allow(::Gitlab::Llm::VertexAi::Configuration)
        .to receive(:payload_parameters)
        .with(temperature: 0)
        .and_return(payload_parameters)
    end

    context 'when specific diff_id does not exist' do
      let(:diff_id) { mr_diff.id + 1 }

      it 'does not make a request to AI provider' do
        expect(Gitlab::Llm::VertexAi::Client).not_to receive(:new)

        subject.execute
      end
    end

    context 'when the text client returns a successful response' do
      let(:example_answer) { 'Summary generated by AI' }
      let(:example_response) do
        {
          "predictions" => [
            {
              "candidates" => [
                {
                  "author" => "",
                  "content" => example_answer
                }
              ],
              "safetyAttributes" => {
                "categories" => ["Violent"],
                "scores" => [0.4000000059604645],
                "blocked" => false
              }
            }
          ]
        }
      end

      before do
        allow_next_instance_of(Gitlab::Llm::VertexAi::Client, user, tracking_context: tracking_context) do |client|
          allow(client)
            .to receive(:text)
            .with(content: prompt, parameters: payload_parameters)
            .and_return(example_response.to_json)
        end
      end

      it 'stores the content from the AI response' do
        expect { subject.execute }
          .to change { ::MergeRequest::DiffLlmSummary.count }
          .by(1)

        diff_llm_summary = mr_diff.merge_request_diff_llm_summary

        aggregate_failures do
          expect(diff_llm_summary.merge_request_diff).to eq(mr_diff)
          expect(diff_llm_summary.provider).to eq('vertex_ai')
          expect(diff_llm_summary.content).to eq('Summary generated by AI')
        end
      end

      context 'when the AI response is too big' do
        let(:example_answer) { "i" * 3000 }

        it 'does not store the content' do
          expect { subject.execute }
            .not_to change { ::MergeRequest::DiffLlmSummary.count }
        end

        it 'returns unsaved Active Record object' do
          response = subject.execute

          expect(response).to be_a(MergeRequest::DiffLlmSummary)
          expect(response).not_to be_persisted
        end

        it 'does not raise an error' do
          expect { subject.execute }
            .not_to raise_error
        end
      end
    end

    context 'when the text client returns an unsuccessful response' do
      let(:error) { { error: { message: 'Error' } } }

      before do
        allow_next_instance_of(Gitlab::Llm::VertexAi::Client, user, tracking_context: tracking_context) do |client|
          allow(client)
            .to receive(:text)
            .with(content: prompt, parameters: payload_parameters)
            .and_return(error.to_json)
        end
      end

      it 'does not store the content' do
        expect { subject.execute }
          .not_to change { ::MergeRequest::DiffLlmSummary.count }
      end
    end

    context 'when the AI response is empty' do
      before do
        allow_next_instance_of(Gitlab::Llm::VertexAi::Client, user, tracking_context: tracking_context) do |client|
          allow(client)
            .to receive(:text)
            .with(content: prompt, parameters: payload_parameters)
            .and_return({})
        end
      end

      it 'does not store the content' do
        expect { subject.execute }
          .not_to change { ::MergeRequest::DiffLlmSummary.count }
      end

      it 'does not raise an error' do
        expect { subject.execute }
          .not_to raise_error
      end
    end
  end
end
